{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "bert-beginning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShenghanZhang/Blog-Back-Up/blob/master/Tweet%20Disaster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utxsb-4-ADKv",
        "outputId": "2c929e16-2fa8-4103-8b83-d255141383d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l1NAvfNAE8Z"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/Disaster Tweets')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": false,
        "trusted": true,
        "id": "ud8YohzXwRRk"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import bert_tokenization as tokenization\n",
        "import tensorflow.keras.backend as K\n",
        "import gc\n",
        "import os\n",
        "from scipy.stats import spearmanr\n",
        "from math import floor, ceil\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "np.set_printoptions(suppress=True)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mLV-co3DwRRo",
        "outputId": "848832ee-4fb1-409e-c0db-46596ffd4e02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "PATH = '/content/drive/My Drive/Colab Notebooks/Disaster Tweets/inputs/'\n",
        "BERT_PATH = '/content/drive/My Drive/Colab Notebooks/Disaster Tweets/bert_en_uncased_L-12_H-768_A-12'\n",
        "tokenizer = tokenization.FullTokenizer(BERT_PATH+'/assets/vocab.txt', True)\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "\n",
        "df_train = pd.read_csv(PATH+'train_data_cleaning.csv')\n",
        "df_test = pd.read_csv(PATH+'test_data_cleaning.csv')\n",
        "\n",
        "df_train = df_train\n",
        "df_test = df_test\n",
        "print('train shape =', df_train.shape)\n",
        "print('test shape =', df_test.shape)\n",
        "print('train columns is {}'.format(df_train.columns))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape = (7613, 5)\n",
            "test shape = (3263, 4)\n",
            "train columns is Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uY_tQ2_ZwRRq"
      },
      "source": [
        "def _get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def _get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    segments = []\n",
        "    first_sep = True\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            if first_sep:\n",
        "                first_sep = False \n",
        "            else:\n",
        "                current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def _get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids\n",
        "\n",
        "\n",
        "def _convert_to_bert_inputs(text,tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
        "    \n",
        "    stoken = [\"[CLS]\"] + tokenizer.tokenize(text) + [\"[SEP]\"]\n",
        "\n",
        "    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
        "    input_masks = _get_masks(stoken, max_sequence_length)\n",
        "    input_segments = _get_segments(stoken, max_sequence_length)\n",
        "\n",
        "    return [input_ids, input_masks, input_segments]\n",
        "\n",
        "def compute_input_arrays(df, tokenizer, max_sequence_length):\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for i in tqdm(range(len(df))):\n",
        "        t = df.iloc[i].text\n",
        "        ids, masks, segments = _convert_to_bert_inputs(t, tokenizer, max_sequence_length)\n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "        \n",
        "    return [np.asarray(input_ids, dtype=np.int32), \n",
        "            np.asarray(input_masks, dtype=np.int32), \n",
        "            np.asarray(input_segments, dtype=np.int32)]\n",
        "\n",
        "\n",
        "def compute_output_arrays(df):\n",
        "    return np.asarray(df.target)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "izzO6MKvwRRs"
      },
      "source": [
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n",
        "\n",
        "        self.valid_inputs = valid_data[0]\n",
        "        self.valid_outputs = valid_data[1]\n",
        "        self.test_inputs = test_data\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.fold = fold\n",
        "\n",
        "        self.valid_predictions = []\n",
        "        self.val_precision_scores = []\n",
        "        self.val_recall_scores = []\n",
        "        self.val_f1_scores = []\n",
        "\n",
        "        self.test_predictions = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.valid_predictions.append(\n",
        "            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n",
        "\n",
        "        valid_pred_labels = np.round(self.valid_predictions)[-1]\n",
        "\n",
        "        self.val_precision_scores.append(precision_score(self.valid_outputs, valid_pred_labels , average='macro'))\n",
        "        self.val_recall_scores.append(recall_score(self.valid_outputs, valid_pred_labels , average='macro'))\n",
        "        self.val_f1_scores.append(f1_score(self.valid_outputs, valid_pred_labels , average='macro'))\n",
        "\n",
        "        self.test_predictions.append(\n",
        "            self.model.predict(self.test_inputs, batch_size=self.batch_size))\n",
        "\n",
        "\n",
        "        print('\\nEpoch: {} - Validation Precision: {:.6} - Validation Recall: {:.6} - Validation '\n",
        "              'F1: {:.6}'.format(epoch, self.val_precision_scores[-1], self.val_recall_scores[-1],self.val_f1_scores[-1]))\n",
        "        \n",
        "        \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "odsWeL10wRRu"
      },
      "source": [
        "def bert_model():\n",
        "    \n",
        "    input_word_ids = tf.keras.layers.Input(\n",
        "        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n",
        "    input_masks = tf.keras.layers.Input(\n",
        "        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n",
        "    input_segments = tf.keras.layers.Input(\n",
        "        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n",
        "    \n",
        "    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\n",
        "    \n",
        "    _, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n",
        "    \n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_output\")(x)\n",
        "\n",
        "    model = tf.keras.models.Model(\n",
        "        inputs=[input_word_ids, input_masks, input_segments], outputs=out)\n",
        "    \n",
        "    return model  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6PIoteq1wRRw"
      },
      "source": [
        "def train_and_predict(model, train_data, valid_data, test_data, \n",
        "                      learning_rate, epochs, batch_size, loss_function, fold):\n",
        "\n",
        "    custom_callback = CustomCallback(\n",
        "        valid_data=(valid_data[0], valid_data[1]),\n",
        "        test_data=test_data,\n",
        "        batch_size=batch_size,\n",
        "        fold=fold)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss=loss_function, optimizer=optimizer)\n",
        "    model.fit(train_data[0], train_data[1], epochs=epochs,\n",
        "              batch_size=batch_size,callbacks= [custom_callback] )\n",
        "    \n",
        "    return custom_callback"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XbqtSM6qwRRy",
        "outputId": "b9d04e59-6ab7-48ef-ab87-e7b3686b28ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "outputs = compute_output_arrays(df_train)\n",
        "inputs = compute_input_arrays(df_train, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "test_inputs = compute_input_arrays(df_test, tokenizer, MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7613/7613 [00:03<00:00, 2278.41it/s]\n",
            "100%|██████████| 3263/3263 [00:01<00:00, 2331.41it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "J7lkHyFEwRR1"
      },
      "source": [
        "skf = StratifiedKFold(n_splits=5,random_state=116,shuffle=True).split(X=df_train.text, y = df_train.target)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "R1wrL86UwRR3",
        "outputId": "9d4dac04-e0c4-4a3a-e42e-e38c9208fe51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "histories = []\n",
        "for fold, (train_idx, valid_idx) in enumerate(skf):\n",
        "    \n",
        "    # will actually only do 3 folds (out of 5) to manage < 2h\n",
        "    if fold > 2:\n",
        "        K.clear_session()\n",
        "        model = bert_model()\n",
        "\n",
        "        train_inputs = [inputs[i][train_idx] for i in range(3)]\n",
        "        train_outputs = outputs[train_idx]\n",
        "\n",
        "        valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n",
        "        valid_outputs = outputs[valid_idx]\n",
        "\n",
        "        # history contains two lists of valid and test preds respectively:\n",
        "        #  [valid_predictions_{fold}, test_predictions_{fold}]\n",
        "        history = train_and_predict(model, \n",
        "                          train_data=(train_inputs, train_outputs), \n",
        "                          valid_data=(valid_inputs, valid_outputs),\n",
        "                          test_data=test_inputs, \n",
        "                          learning_rate=3e-5, epochs=4, batch_size=8,\n",
        "                          loss_function='binary_crossentropy', fold=fold)\n",
        "\n",
        "        histories.append(history)\n",
        "        \n",
        "        pd.DataFrame(index = valid_idx, data = {'valid_pred':histories[-1].valid_predictions[-1].reshape(-1)}).to_csv(PATH+str(fold)+'train_pred.csv')\n",
        "        pd.DataFrame(data = {'test_pred': histories[-1].test_predictions[-1].reshape(-1)}).to_csv(PATH+str(fold)+'test_pred.csv')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "762/762 [==============================] - ETA: 0s - loss: 0.4310\n",
            "Epoch: 0 - Validation Precision: 0.830542 - Validation Recall: 0.816847 - Validation F1: 0.821186\n",
            "762/762 [==============================] - 1005s 1s/step - loss: 0.4310\n",
            "Epoch 2/4\n",
            "762/762 [==============================] - ETA: 0s - loss: 0.2404\n",
            "Epoch: 1 - Validation Precision: 0.802086 - Validation Recall: 0.804058 - Validation F1: 0.802941\n",
            "762/762 [==============================] - 1004s 1s/step - loss: 0.2404\n",
            "Epoch 3/4\n",
            "762/762 [==============================] - ETA: 0s - loss: 0.1059\n",
            "Epoch: 2 - Validation Precision: 0.81688 - Validation Recall: 0.795262 - Validation F1: 0.800692\n",
            "762/762 [==============================] - 1001s 1s/step - loss: 0.1059\n",
            "Epoch 4/4\n",
            "762/762 [==============================] - ETA: 0s - loss: 0.0709\n",
            "Epoch: 3 - Validation Precision: 0.811058 - Validation Recall: 0.80055 - Validation F1: 0.804048\n",
            "762/762 [==============================] - 1000s 1s/step - loss: 0.0709\n",
            "Epoch 1/4\n",
            "762/762 [==============================] - ETA: 0s - loss: 0.4384\n",
            "Epoch: 0 - Validation Precision: 0.833582 - Validation Recall: 0.828461 - Validation F1: 0.83055\n",
            "762/762 [==============================] - 1002s 1s/step - loss: 0.4384\n",
            "Epoch 2/4\n",
            "762/762 [==============================] - ETA: 0s - loss: 0.2557\n",
            "Epoch: 1 - Validation Precision: 0.829757 - Validation Recall: 0.827476 - Validation F1: 0.828507\n",
            "762/762 [==============================] - 1003s 1s/step - loss: 0.2557\n",
            "Epoch 3/4\n",
            "762/762 [==============================] - ETA: 0s - loss: 0.1172\n",
            "Epoch: 2 - Validation Precision: 0.826857 - Validation Recall: 0.817391 - Validation F1: 0.820734\n",
            "762/762 [==============================] - 1004s 1s/step - loss: 0.1172\n",
            "Epoch 4/4\n",
            "762/762 [==============================] - ETA: 0s - loss: 0.0771\n",
            "Epoch: 3 - Validation Precision: 0.829458 - Validation Recall: 0.810752 - Validation F1: 0.815984\n",
            "762/762 [==============================] - 1004s 1s/step - loss: 0.0771\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}